---
title: "PSET6"
author: "Ethan Mathieu"
date: "2024-04-07"
output: html_document
---

```{r}

install.packages("vegan")
install.packages("vegan3d")

```

```{r}

library(dplyr)
library(vegan)
library(vegan3d)
library(mgcv)
library(MASS)
library(rgl)
library(ggplot2)
library(tidyverse)

```

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

eddata <- read.csv("Global_Education.csv")

```

## Question 1 

``` {r}

eddata_leveled <- eddata |>
  mutate(literacy_level = case_when(
    (Youth_15_24_Literacy_Rate_Male + Youth_15_24_Literacy_Rate_Female) / 2 >= 70 ~ "HL",
    (Youth_15_24_Literacy_Rate_Male + Youth_15_24_Literacy_Rate_Female) >= 10 ~ "ML",
    TRUE ~ "LL"
  )) |>
  mutate(unemployment_level = case_when(
    Unemployment_Rate >= 8 ~ "HU",
    Unemployment_Rate >= 4 ~ "MU",
    TRUE ~ "LU"
  )) |> 
  mutate(birth_level = case_when(
    Birth_Rate >= 40 ~ "HBL",
    Birth_Rate >= 20 ~ "MBL",
    TRUE ~ "LBL"
  )) |>
  mutate(education_level = case_when(
    (Completion_Rate_Upper_Secondary_Male + Completion_Rate_Upper_Secondary_Female) / 2 >= 70 ~ "HEL",
    (Completion_Rate_Upper_Secondary_Male + Completion_Rate_Upper_Secondary_Female) / 2 >= 40 ~ "MEL",
    TRUE ~ "LEL"
  )) 

eddata_selected <- eddata_leveled |>
  mutate(literacy_unemployment_level = paste(literacy_level, unemployment_level, sep = "_")) |>
  dplyr::select(Countries.and.areas, literacy_unemployment_level, birth_level, education_level, Gross_Tertiary_Education_Enrollment)

count_data <- eddata_selected |>
  group_by(literacy_unemployment_level, birth_level, education_level) |>
  summarise(count = n(), .groups = "drop") |>
  ungroup()

pivot_data <- count_data |>
  pivot_wider(names_from = c(birth_level, education_level), values_from = count, values_fill = 0)

pivot_data <- pivot_data |>
  mutate(literacy_unemployment_level = factor(literacy_unemployment_level, levels = c("HL_HU", "HL_MU", "HL_LU", "ML_HU", "ML_MU", "ML_LU", "LL_HU", "LL_MU", "LL_LU"))) |>
  arrange(literacy_unemployment_level)


```

To prepare the date for correspondence analysis, it was necessary to construct a
contingency table. This data contains information, by country, about various
societal variables. These are rates. For instance, Completion_Rate_Upper_Secondary_Female
variable indicates what percent of women complete secondary school. To create a contingency table,
we fixed categories to a few key rates. If the rate exceeded a certain threshold, we noted it
as "high", "medium" or "low" respectively. Theis allows us to count the number of nations that
fit into certain categories, priming our creation of a contigency table.

To create the actual contigency table, we isolated four of these cateogized variables: Unemployment Level,
Education Level, Birth Level and and Literacy Level. We combined Literacy Level and Unemployment 
Level into a compound category as well as Education level and Birth Level. We counted
how many nations feel into each combination fo these compound categories. 

ex: The number of nations that were High Literacy Level + High Unemployment Level as well as High Education Level + High Birth Level

This is condified as HL_HU + HEL_HBL, so that it can appear well on a plot.

## Question 2

``` {r}

eddata_cca <- cca(pivot_data[, -1])
eddata_cca

```

As we can see, the majority of the inertia is indeed explained by CA1 and CA2. They account for over 
73% of the total inertia, or variability, of the dataset. Let's take a look 
at the plot.

``` {r}

plot(eddata_cca, type = "n" , xlim = c(-.5, .5))
text(eddata_cca, dis = "wa", labels =  c("HL_HU", "HL_MU", "HL_LU", "ML_HU", "ML_MU", "ML_LU", "LL_HU", "LL_MU", "LL_LU"))
points(eddata_cca, pch = 21, col = "red", bg = "yellow", cex = 1.2)
text(eddata_cca, "species", col = "blue", cex = 0.8)

```


## Question 3 & 4
There is no evidence of data snaking in higher dimensional spaces. This makes sense
because there is no natural "handoff" that occurs in that data, where one distribution
bleeds into another. Rather, the data clusters around certain logical associations.

For instance, High Literacy Level and Low Unemployment has strong correspondence with
Medium Birth Level and High Education Level, which makes sense. There are certain surprising conclusions, like the fact that High Education Level and Low Birth Level has a strong correspondence with Low Literacy Level and High Unemplyoment. However, there are some combinations that seem to have little correspondence with anything, such as medium literacy level and high unemployment level. The straight inverse combinations 
(High-X and Low-Y) seem to have the most correspondence power.

## Question 5
``` {r}

pivot_data <- pivot_data %>%
  mutate_all(as.numeric)  
num_dimensions <- 5  

results <- matrix(NA, nrow = 21, ncol = num_dimensions)

for (j in 1:num_dimensions) {
  for (i in 1:8) {
    temp <- pivot_data[sample(nrow(pivot_data)), 1] 
    for (k in 1:8) { 
      temp <- cbind(temp, pivot_data[sample(nrow(pivot_data)), k]) 
    }
    mds_result <- metaMDS(temp, k = j, distance = "euclidean")
    results[i, j] <- mds_result$stress
  }
  mds_result <- metaMDS(pivot_data, k = j, distance = "euclidean")
  results[21, j] <- mds_result$stress
}







```

Here we run multidimensional scaling with several different dimesnions. We do one 
original MDS, and 20 other permuations with shuffled data. The stress value
for each dimension is calculated and stored. We plot these results, showing the original 
run agaisnt each of the permutations. Teaching Assistants stated that the scree
plot contained in the next question is sufficient plotting for this question.

## Question 6

``` {r}

plot(c(1:num_dimensions), results[21, ], type = "b", col = "blue", lwd = 3, 
     ylim = c(0, max(results, na.rm = TRUE)), xlab = "Dimensions", ylab = "Stress", pch = 19, 
     main = "MDS Stress and Permutation Results")
mins <- apply(results[1:20, ], 2, min, na.rm = TRUE)
maxs <- apply(results[1:20, ], 2, max, na.rm = TRUE)
meds <- apply(results[1:20, ], 2, median, na.rm = TRUE)

for (i in 1:num_dimensions) {
  points(rep(i, 3), c(mins[i], meds[i], maxs[i]), type = "b", col = "red", lwd = 3, pch = 19)
}

legend("topright", legend = c("MDS Solution", "20 Permutations"), lwd = 3, col = c("blue", "red"))

```

Knowing that the blue line is the actual line measured stress while the red
dots are what we would expect to happen just by chance, we not that the line
begins to smooth out around dimension 2. In other words, the stress value, or the
difference between the high dimension points and the low dimesnional approximation, begins 
to flatten out at around dimension 2. This means that 2 dimensions is a pretty good
projection of the original high dimension data; projecting on more dimensions is
seemingly unncessary. 

## Question 7
``` {r}

mds_2d <- metaMDS(pivot_data, k = 2, distance = "euclidean")
fig <- ordiplot(mds_2d, type = "none", cex = 1.1)
text(fig, "species", col = "red", cex = 1.1)
text(fig, "sites", col = "blue", cex = 0.8)

```

## Question 8

``` {r}
env <- eddata_leveled |>
  mutate(literacy_unemployment_level = paste(literacy_level, unemployment_level, sep = "_")) |>
  group_by(literacy_unemployment_level, birth_level, education_level) |>
  summarise(avg = mean(Gross_Tertiary_Education_Enrollment)) |>
  group_by(literacy_unemployment_level) |>
  summarise(college_completion_avg = mean(avg))

env
```

Here, we create a continious variable for each of the categories we described earlier. It's the average
college completion rate for each of the categorical combinations. We have included the head of the
data, so that is more obvious what is happening. Also, to make the stress calculations work
we needed to convert the row categories into numbers. Here are the mappings:

1 -> High Literacy Level + High Unemployment Level
2 -> High Literacy Level + Medium Unemployment Level
...
9 -> Low Literacy Level + Low Unemployment Level



``` {r}
fig <- ordiplot(mds_2d, type = "none", cex = 1.1)
fit <- envfit(mds_2d, env, permutations = 1000)
plot(fit, col = "red", lwd = 3)
fit
```

Here we see college_completion_avg point generally in the direction of High Literacy Rate combinations.
Note: It was incredibly difficult wrangling the data to include a continious variable, as our 
data is not very well suited for it due to it not being an originally contingent dataset.

## Question 9

``` {r}

(canon <- cca(pivot_data, env, scale = "FALSE"))

```

## Question 10


